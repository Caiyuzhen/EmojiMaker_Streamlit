import streamlit as st
from src.Config import Config
import requests
# from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


INFO = "â„¹ï¸ Open AI key æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥"


def request_llm_custom_msg(
	api_key,
	inputMsg, # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
	temperature,
	server_url, # æœåŠ¡ URL
	model,
	max_tokens,
):
	# ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åŠ è½½æ¨¡å‹
	llm = ChatOpenAI(
		base_url=server_url, 
		api_key=api_key,
		temperature=temperature,
	)

	# æ„é€ æç¤ºè¯æ¨¡æ¿
	template = """
		ä½ æ˜¯ä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œä½ çš„ä»»åŠ¡æ˜¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

		ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š
		â€œ{{question}}â€ã€‚
			
		è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜, å¹¶åœ¨æ¯æ¬¡å›ç­”åéƒ½è¯´ â€œå•¦å•¦å•¦æˆ‘å›ç­”å®Œå•¦ï¼â€
	"""

	# ç”Ÿæˆæç¤ºè¯	
	prompt = PromptTemplate(template)
 
	# æ„é€ è¾“å‡ºè§£æå™¨å’Œã€é“¾ã€‘
	output_parser = StrOutputParser()
 
	# è°ƒç”¨é“¾, è·å–å›ç­”
	chain = prompt | llm | output_parser


	response = chain.invoke({"question": inputMsg})
	st.info(response)
 
	# è¿”å›æ¨¡å‹çš„å›ç­”
	return response, "OK ğŸ‘"

	# try:
	# 	completion = llm.chat.completions.create(
	# 		api_key=api_key,
	# 		temperature=temperature, # æ¨¡å‹è¾“å‡ºçš„åˆ›æ„ç¨‹åº¦ (éšæœºæ€§)
	# 		server_url=server_url,
	# 		model=model,
	# 		messages=[
	# 			{"role": "system", "content": "Please answer the user's question."},
	# 			{"role": "user", "content": inputMsg},
	# 		],
	# 		max_tokens=max_tokens,
	# 	)
	# 	# è¿”å›æ¨¡å‹çš„å›ç­”
	# 	return completion.choices[0].message['content'], "OK ğŸ‘"
	# except Exception as e:
	# 		return f"Error: {str(e)}", "â›”"