import streamlit as st
from src.Config import Config
import requests
# from openai import OpenAI
from langchain.llms import
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
# from llamaapi import LlamaAPI


INFO = "â„¹ï¸ Open AI key æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥"


def request_llm_custom_msg(
	api_key,
	inputMsg, # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
	temperature,
	server_url, # æœåŠ¡ URL
	model,
	max_tokens,
):
	# ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åŠ è½½æ¨¡å‹
	chat = ChatOpenAI(
		base_url="http://127.0.0.1:1234/v1", 
		api_key="EMPTY",
		temperature=0.7,
	)
	# res = chat.invoke("ä½ å¥½, è¯·å‘Šè¯‰æˆ‘å¦‚ä½•åšä¸€ä¸ªå®Œç¾çš„è·åŒ…è›‹")
	# print(res)
 


	# ç”Ÿæˆæç¤ºè¯	
	# prompt = ChatPromptTemplate.from_template(template=template)
	chat_template_prompt = ChatPromptTemplate.from_messages(
		[
			("system", "ä½ æ˜¯ä¸€ä¸ªå¨å¸ˆ, ä½ çš„ä»»åŠ¡æ˜¯å›ç­”ç”¨æˆ·çš„é—®é¢˜, è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"),
			("human", "Hello, how are you doing?"),
			("ai", "I'm doing well, thanks!"),
			("human", "{user_input}"), # ğŸ‘ˆ æ„å»ºåŠ¨æ€çš„ç”¨æˆ·è¾“å…¥å†…å®¹
		]
	)
 
	# è¾“å‡ºè§£æå™¨å’Œã€é“¾ã€‘
	output_parser = StrOutputParser()
 
	# æ„å»º LCEL é“¾
	chain = (
		chat_template_prompt |
		chat |
		output_parser
	)

	# åŠ¨æ€åœ°å®šä¹‰ç”¨æˆ·çš„é—®é¢˜
	user_input = "ä½ å¥½, è¯·æ•™æˆ‘æ€ä¹ˆåšç‚’é¸¡è›‹è¿™é“èœ, ç”¨ä¸­æ–‡å›ç­”æˆ‘"



	# è°ƒç”¨é“¾å¹¶ä¼ é€’ user_input (ğŸŒŸ ä¸€èˆ¬è¾“å‡º) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
	# response = chain.invoke({"user_input": user_input})
	# print(response)
 
 
    # ä½¿ç”¨æµå¼è°ƒç”¨ (ğŸŒŸ æµå¼è¾“å‡º) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
	for chunk in chain.stream({"user_input": user_input}):
		print(chunk, end="")
  
  
  
  
	# ğŸ‘‡ OpenAI Python SDK çš„è°ƒç”¨æ–¹å¼
	# try:
	# 	completion = llm.chat.completions.create(
	# 		api_key=api_key,
	# 		temperature=temperature, # æ¨¡å‹è¾“å‡ºçš„åˆ›æ„ç¨‹åº¦ (éšæœºæ€§)
	# 		server_url=server_url,
	# 		model=model,
	# 		messages=[
	# 			{"role": "system", "content": "Please answer the user's question."},
	# 			{"role": "user", "content": inputMsg},
	# 		],
	# 		max_tokens=max_tokens,
	# 	)
	# 	# è¿”å›æ¨¡å‹çš„å›ç­”
	# 	return completion.choices[0].message['content'], "OK ğŸ‘"
	# except Exception as e:
	# 		return f"Error: {str(e)}", "â›”"